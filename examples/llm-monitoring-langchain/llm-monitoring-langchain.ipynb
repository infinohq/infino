{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d10861f-a550-4443-bc63-4ce2ae13b841",
   "metadata": {},
   "source": [
    "# Infino - LangChain LLM Monitoring Example\n",
    "\n",
    "This example shows how one can track the following while calling OpenAI models via LangChain:\n",
    "\n",
    "* prompt input,\n",
    "* response from chatgpt or any other LangChain model,\n",
    "* latency,\n",
    "* errors,\n",
    "* number of tokens consumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5a0976-9953-41d8-880c-eb3f2992e936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: infinopy in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.0.1)\n",
      "Requirement already satisfied: docker in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from infinopy) (6.1.3)\n",
      "Requirement already satisfied: requests in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from infinopy) (2.31.0)\n",
      "Requirement already satisfied: packaging>=14.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from docker->infinopy) (23.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from docker->infinopy) (2.0.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from docker->infinopy) (1.5.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->infinopy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->infinopy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vinaykakade/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->infinopy) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Install necessary dependencies.\n",
    "!pip install matplotlib\n",
    "\n",
    "sys.path.append(\"../../../../langchain\")\n",
    "#!pip install langchain\n",
    "\n",
    "!pip install infinopy\n",
    "\n",
    "import datetime as dt\n",
    "from infinopy import InfinoClient\n",
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import InfinoCallbackHandler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90210d-c805-4a0c-81e4-d5298942afc4",
   "metadata": {},
   "source": [
    "## Start Infino server, initialize the Infino client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748b9858-5145-4351-976a-ca2d54e836a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1bd7d32f1ed0d319728a2b454e9078d264e2e7aa5e07d8daae1f6f68c78bc3cd\n"
     ]
    }
   ],
   "source": [
    "# Start server using the Infino docker image.\n",
    "!docker run --rm --detach --name infino-example -p 3000:3000 infinohq/infino:latest\n",
    "\n",
    "# Create Infino client.\n",
    "client = InfinoClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b81cda-b841-43ee-8c5e-b1576555765f",
   "metadata": {},
   "source": [
    "## Read the questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b659fd0c-0d8c-470e-8b6c-867a117f2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the questions. These are a subset of questions from Stanford's QA dataset - \n",
    "# https://rajpurkar.github.io/SQuAD-explorer/\n",
    "fh = open(\"../datasets/stanford-qa-subset.txt\")\n",
    "lines = fh.readlines()\n",
    "fh.close()\n",
    "\n",
    "questions = []\n",
    "for line in lines:\n",
    "  if not line.startswith(\"#\"):\n",
    "    questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1b820-3f1a-4b94-b848-4c6032cadc18",
   "metadata": {},
   "source": [
    "## LangChain OpenAI Q&A; Publish metrics and logs to Infino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cebf35-2d10-48b8-ab11-c4a574c595d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StdOutCallbackHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-KlQlX2wt2JUvpXjybAj7T3BlbkFJviaDo0JLL95GZ64fp7V3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create LLM.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[43mStdOutCallbackHandler\u001b[49m()\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Number of questions to ask the OpenAI model. We limit to a short number here to save $$ while running this demo.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StdOutCallbackHandler' is not defined"
     ]
    }
   ],
   "source": [
    "# Set your key here.\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Create LLM.\n",
    "handler = InfinoCallbackHandler()\n",
    "llm = OpenAI(temperature=0.1, verbose=True)\n",
    "\n",
    "# Number of questions to ask the OpenAI model. We limit to a short number here to save $$ while running this demo.\n",
    "num_questions = 1\n",
    "\n",
    "questions = questions[0:num_questions]\n",
    "question_id = 0\n",
    "for question in questions:\n",
    "    payload = {\"date\": int(time.time()), \"prompt_question\": question, \"labels\": {\"id\": question_id}}\n",
    "    client.append_log(payload)\n",
    "    \n",
    "    start = time.time()\n",
    "    print(question)\n",
    "\n",
    "    is_error = 0\n",
    "    try:\n",
    "      # We send the question to OpenAI api and ask to limit to 2 sentences, mainly to save $$.\n",
    "      llm_result = llm.generate([question], callbacks=[handler])\n",
    "      print(llm_result)\n",
    "    except Exception as e:\n",
    "      is_error = 1\n",
    "      print(\"Error from OpenAI: \", e)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    payload = {\"date\": int(time.time()), \"latency\": duration, \"labels\": {\"id\": question_id}}\n",
    "    client.append_ts(payload)\n",
    "    \n",
    "    payload = {\"date\": int(time.time()), \"error\": is_error, \"labels\": {\"id\": question_id}}\n",
    "    client.append_ts(payload)\n",
    "\n",
    "    if not is_error:\n",
    "      generation = llm_result.generations[0][0]\n",
    "      llm_output = llm_result.llm_output\n",
    "      payload = {\"date\": int(time.time()), \"prompt_response\": generation.text, \"labels\": {\"id\": question_id}}\n",
    "      print(\"Indexing payload\", payload)\n",
    "      client.append_log(payload)\n",
    "      \n",
    "      payload = {\"date\": int(time.time()), \"prompt_tokens\": llm_output['token_usage']['prompt_tokens'], \"labels\": {\"id\": question_id}}\n",
    "      client.append_ts(payload)\n",
    "      \n",
    "      payload = {\"date\": int(time.time()), \"completion_tokens\": llm_output['token_usage']['completion_tokens'], \"labels\": {\"id\": question_id}}\n",
    "      client.append_ts(payload)\n",
    "      \n",
    "      payload = {\"date\": int(time.time()), \"total_tokens\": llm_output['token_usage']['total_tokens'], \"labels\": {\"id\": question_id}}\n",
    "      client.append_ts(payload)\n",
    "\n",
    "    question_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ec697-c922-4fd9-aad1-f49c6ac24e8a",
   "metadata": {},
   "source": [
    "## Create Metric Charts\n",
    "\n",
    "We now use matplotlib to create graphs of latency, errors and tokens consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078c612-89e0-4a1d-b1a8-bf36b664a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a graph using matplotlib.\n",
    "def plot(data, title):\n",
    "  data = json.loads(data)\n",
    "\n",
    "  # Extract x and y values from the data\n",
    "  timestamps = [item[\"time\"] for item in data]\n",
    "  dates=[dt.datetime.fromtimestamp(ts) for ts in timestamps]\n",
    "  y = [item[\"value\"] for item in data]\n",
    "\n",
    "  plt.rcParams['figure.figsize'] = [6, 4]\n",
    "  plt.subplots_adjust(bottom=0.2)\n",
    "  plt.xticks(rotation=25 )\n",
    "  ax=plt.gca()\n",
    "  xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\n",
    "  ax.xaxis.set_major_formatter(xfmt)\n",
    "  \n",
    "  # Create the plot\n",
    "  plt.plot(dates, y)\n",
    "\n",
    "  # Set labels and title\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(\"Value\")\n",
    "  plt.title(title)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "response = client.search_ts(\"__name__\", \"latency\", 0, int(time.time()))\n",
    "plot(response.text, \"Latency\")\n",
    "\n",
    "response = client.search_ts(\"__name__\", \"error\", 0, int(time.time()))\n",
    "plot(response.text, \"Errors\")\n",
    "\n",
    "response = client.search_ts(\"__name__\", \"prompt_tokens\", 0, int(time.time()))\n",
    "plot(response.text, \"Prompt Tokens\")\n",
    "\n",
    "response = client.search_ts(\"__name__\", \"total_tokens\", 0, int(time.time()))\n",
    "plot(response.text, \"Total Tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d61822-1781-4bc6-97a2-2abc5c2b2e75",
   "metadata": {},
   "source": [
    "## Full text query on prompt or prompt outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f051f0-e2bc-44e7-8dfb-bfd5bbd0fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a particular prompt text.\n",
    "query = \"normandy\"\n",
    "response = client.search_log(query, 0, int(time.time()))\n",
    "print(\"Results for\", query, \":\", response.text)\n",
    "\n",
    "print(\"===\")\n",
    "\n",
    "query = \"king charles III\"\n",
    "response = client.search_log(\"france.\", 0, int(time.time()))\n",
    "print(\"Results for\", query, \":\", response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b171074-c775-48e0-a4b3-f550e2c8eccb",
   "metadata": {},
   "source": [
    "## Step 5: Stop infino server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147663cb-b88f-4cfb-9726-7231dbec7cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infino-example\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f infino-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f36c49-53a3-460d-b74b-995cda7726b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752f8f5-d5c8-4a8c-b5b9-a44f71b56772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
